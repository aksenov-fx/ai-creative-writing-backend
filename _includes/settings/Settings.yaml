# Prompt variables
system_prompt: null
introduction: ""
variables: {}
prompts_structure: {}
abbreviations: {}
translation_language: "English"
history_prefix: "**Previous text parts for reference:**"

# Story parsing settings
separator: "\n----\n"

# Endpoints settings
endpoints: {}
default_endpoint: "openrouter"
endpoint: null

# Models settings
models: {}
default_model: "Deepseek V3.2"
summary_model: "Deepseek V3.2"
model: ""

# Context length settings
max_tokens: 100000
trim_history: false
use_summary: true
include_previous_part_when_summarizing: True
include_previous_part_when_rewriting: True

# API settings
temperature: 0.8

# Output settings
print_messages: true
include_reasoning: true
print_reasoning: true

# Paths
history_path: "Story.md"
summary_yaml_path: "Data/Summary YAML.yaml"
summary_md_path: "Story Summary.md"
prompts_path: "Prompts.md"
folder_path: ""
settings_folder: "./_includes/settings/"

# Runtime flags
interrupt_flag: false
debug: false

# Chat settings that are not used in story mode
splitter: ""
add_header: true
chat_with_story: false
include_file: ""
custom_instructions_folder: ""

# Constants
PORT: 9993
BUFFER_SIZE: 1024
TOKEN_ESTIMATION_DIVISOR: 4
WRITE_INTERVAL: 1.0
RETRY_BASE_DELAY: 0.1
TIMESTAMP_UPDATE_DELAY: 0.3